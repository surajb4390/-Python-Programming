{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95ee21f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (0.1.96)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (4.12.5)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (0.11.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (4.62.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (0.24.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (1.22.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (1.6.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (3.6.2)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (0.1.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (4.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.8.21)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (20.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.47)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.25.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->sentence-transformers) (0.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision->sentence-transformers) (8.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: six in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4dec0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import contractions,re\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize import word_tokenize as w_tokenizer\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca75cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "056bff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Web Scrapped data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ddda403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Machine Learning?</td>\n",
       "      <td>Machine learning is the science of getting co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explain the basic difference between Supervise...</td>\n",
       "      <td>Supervised Learning: A model is trained on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do you mean by Reinforcement Learning?</td>\n",
       "      <td>reinforcement learning is an area of machine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the different types of data used in M...</td>\n",
       "      <td>There Are Two Types of Data. Structured and U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Features vs. Labels?</td>\n",
       "      <td>Features are the input information. On the ot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Questions  \\\n",
       "0                          What is Machine Learning?   \n",
       "1  Explain the basic difference between Supervise...   \n",
       "2        What do you mean by Reinforcement Learning?   \n",
       "3  What are the different types of data used in M...   \n",
       "4                               Features vs. Labels?   \n",
       "\n",
       "                                             Answers  \n",
       "0   Machine learning is the science of getting co...  \n",
       "1  Supervised Learning: A model is trained on the...  \n",
       "2   reinforcement learning is an area of machine ...  \n",
       "3   There Are Two Types of Data. Structured and U...  \n",
       "4   Features are the input information. On the ot...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bdeb7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum no.of.words in a answer :  ['You', 'have', 'to', 'train', 'a', '12GB', 'dataset', 'using', 'a', 'neural', 'network', 'with', 'a', 'machine', 'which', 'has', 'only', '3GB', 'RAM.', 'How', 'would', 'you', 'go', 'about', 'it?']\n",
      "The average length of the answers are : 76\n"
     ]
    }
   ],
   "source": [
    "questions = data['Questions'].to_list()\n",
    "answers = data['Answers'].to_list()\n",
    "print('Maximum no.of.words in a answer : ',max((questions[i].split()) for i in range(len(questions))))\n",
    "print(f'The average length of the answers are : {sum(len(answers[i].split()) for i in range(0,len(answers)))// len(answers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "546c6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a class and creating functions inside the class to perform necessary preprocessing functions\n",
    "\n",
    "class Dataprep:\n",
    "    def __init__(self):\n",
    "        self.correct_cnt = 0\n",
    "        self.incorrect_cnt = 0\n",
    "        self.score = 0\n",
    "        self.model = s_model\n",
    "    \n",
    "    def unicode_to_ascii(self,s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    ## Step 1 and Step 2 \n",
    "    def preprocess_sentence(self,w, answer = False):\n",
    "        #preprocessing sentences\n",
    "        w = self.unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "        w = w.strip()\n",
    "\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "\n",
    "        if answer != False:\n",
    "            w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "    def tokenize(self, sentence, max_len):\n",
    "        # tokenize and add padding to the tokenized sentences\n",
    "\n",
    "        # print(len(lang), \"example sentence: {}\".format(lang[0]))\n",
    "        sent_tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
    "        sent_tokenizer.fit_on_texts(sentence)\n",
    "\n",
    "        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
    "        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
    "        tensor = sent_tokenizer.texts_to_sequences(sentence) \n",
    "\n",
    "        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
    "        ## and pads the sequences to match the longest sequences in the given input\n",
    "        tensor = pad_sequences(tensor, padding='post', maxlen = max_len)\n",
    "\n",
    "        return tensor, sent_tokenizer\n",
    "    def final_data(self, questions, answers):\n",
    "        q_processed = [self.preprocess_sentence(q_) for q_ in questions]\n",
    "        a_processed = [self.preprocess_sentence(a_) for a_ in answers]\n",
    "        \n",
    "        q_tensor, q_tokenizer = self.tokenize(q_processed, 58)\n",
    "        a_tensor, a_tokenizer = self.tokenize(a_processed, 150)\n",
    "        \n",
    "        return q_tensor, q_tokenizer, a_tensor, a_tokenizer\n",
    "    def vocabsize(self, questions, answers):\n",
    "        # finding vocabulary size\n",
    "        q_processed = [self.preprocess_sentence(q_) for q_ in questions]\n",
    "        a_processed = [self.preprocess_sentence(a_) for a_ in answers]\n",
    "        target_regex = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n\\'0123456789'\n",
    "        # Tokenizer allows to vectorize our corpus by turning each sentence\n",
    "        # into a sequence of integers where each integer is an index\n",
    "        # of a token in an internal dictionary\n",
    "        tokenizer = Tokenizer(filters=target_regex)\n",
    "        tokenizer.fit_on_texts(q_processed + a_processed)\n",
    "        VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "        return VOCAB_SIZE\n",
    "        \n",
    "    def call(self):\n",
    "        q_tensor, self.q_tokenizer, a_tensor, self.a_tokenizer = self.final_data(questions, answers)\n",
    "        \n",
    "        return q_tensor, a_tensor, self.q_tokenizer, self.a_tokenizer\n",
    "    \n",
    "    def create_ohe(self, questions, answers):\n",
    "        #creating one hot encoded answers to match with the output\n",
    "        a_processed = [self.preprocess_sentence(a_, answer = True) for a_ in answers]\n",
    "        q_processed = [self.preprocess_sentence(q_) for q_ in questions]\n",
    "        sent_tokenizer = Tokenizer(filters = '', oov_token = '<OOV>')\n",
    "        sent_tokenizer.fit_on_texts(a_processed + q_processed)\n",
    "        token_ans = sent_tokenizer.texts_to_sequences(a_processed)\n",
    "        for i in range(len(token_ans)):\n",
    "            token_ans[i] = token_ans[i][1:]\n",
    "        pad_ans = pad_sequences(token_ans, maxlen = 150, padding = 'post')\n",
    "        dec_out_data = to_categorical(pad_ans,2483) \n",
    "        return dec_out_data\n",
    "    def str_to_tokens(self, sentence: str):\n",
    "        # convert input string to lowercase, \n",
    "        # then split it by whitespaces and then convert it into tokens\n",
    "        tokenizer = Tokenizer(filters = '', oov_token = '<OOV>')\n",
    "        tokenizer.fit_on_texts([self.preprocess_sentence(q_) for q_ in questions] + [self.preprocess_sentence(a_, answer = True) for a_ in answers])\n",
    "        words = sentence.lower().split()\n",
    "        # and then convert to a sequence \n",
    "        # of integers padded with zeros\n",
    "        tokens_list = list()\n",
    "        for current_word in words:\n",
    "            result = tokenizer.word_index.get(current_word, '')\n",
    "            if result != '':\n",
    "                tokens_list.append(result)\n",
    "        return pad_sequences([tokens_list],\n",
    "                             maxlen=58,\n",
    "                             padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "202941a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_creator = Dataprep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "cdbad151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((226, 58), (226, 150))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "enc_input_data, dec_input_data, q_tokenizer, a_tokenizer = data_creator.call()\n",
    "enc_input_data.shape, dec_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8795a751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226, 150, 2483)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_out_data = data_creator.create_ohe(questions, answers)\n",
    "dec_out_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b87bccc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2483"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = data_creator.vocabsize(questions, answers)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc380bdc",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "16f027b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder will be used to capture space-dependent \n",
    "# relations between words from the questions\n",
    "enc_inputs = Input(shape=(None,))\n",
    "enc_embedding = Embedding(vocab_size, 200, mask_zero=True)(enc_inputs)\n",
    "enc_outputs, state_h, state_c = LSTM(200, return_state=True)(enc_embedding)\n",
    "enc_states = [state_h, state_c]\n",
    "# decoder will be used to capture space-dependent relations \n",
    "# between words from the answers using encoder's \n",
    "# internal state as a context\n",
    "dec_inputs = Input(shape=(None,))\n",
    "dec_embedding = Embedding(vocab_size, 200, mask_zero=True)(dec_inputs)\n",
    "dec_lstm = LSTM(200, return_state=True, return_sequences=True)\n",
    "dec_outputs, _, _ = dec_lstm(dec_embedding,  \n",
    "                             initial_state=enc_states)\n",
    "# decoder is connected to the output Dense layer\n",
    "dec_dense = Dense(vocab_size, activation=softmax)\n",
    "output = dec_dense(dec_outputs)\n",
    "model = Model([enc_inputs, dec_inputs], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "16320811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_31 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_32 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_16 (Embedding)       (None, None, 200)    496600      ['input_31[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_17 (Embedding)       (None, None, 200)    496600      ['input_32[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_16 (LSTM)                 [(None, 200),        320800      ['embedding_16[0][0]']           \n",
      "                                 (None, 200),                                                     \n",
      "                                 (None, 200)]                                                     \n",
      "                                                                                                  \n",
      " lstm_17 (LSTM)                 [(None, None, 200),  320800      ['embedding_17[0][0]',           \n",
      "                                 (None, 200),                     'lstm_16[0][1]',                \n",
      "                                 (None, 200)]                     'lstm_16[0][2]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, None, 2483)   499083      ['lstm_17[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,133,883\n",
      "Trainable params: 2,133,883\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = RMSprop(), loss = 'categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2348f529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 5s 658ms/step - loss: 2.1278\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 5s 643ms/step - loss: 2.1154\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 5s 644ms/step - loss: 2.0893\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 5s 642ms/step - loss: 2.0662\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 6s 693ms/step - loss: 2.0495\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 6s 733ms/step - loss: 2.0223\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 6s 691ms/step - loss: 2.0061\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 5s 660ms/step - loss: 1.9791\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 6s 744ms/step - loss: 1.9710\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 6s 725ms/step - loss: 1.9375\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 6s 723ms/step - loss: 1.9176\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 6s 718ms/step - loss: 1.9064\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 6s 756ms/step - loss: 1.8778\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 6s 717ms/step - loss: 1.8588\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 6s 717ms/step - loss: 1.8344\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 6s 737ms/step - loss: 1.8201\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 6s 741ms/step - loss: 1.7994\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 6s 721ms/step - loss: 1.7737\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 6s 723ms/step - loss: 1.7588\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 7s 824ms/step - loss: 1.7466\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 6s 757ms/step - loss: 1.7221\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 6s 764ms/step - loss: 1.6992\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 5s 665ms/step - loss: 1.6903\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 5s 676ms/step - loss: 1.6686\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 6s 687ms/step - loss: 1.6512\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 5s 655ms/step - loss: 1.6241\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 6s 716ms/step - loss: 1.6199\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 5s 659ms/step - loss: 1.6002\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 5s 655ms/step - loss: 1.5727\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 5s 663ms/step - loss: 1.5543\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 5s 663ms/step - loss: 1.5567\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 5s 669ms/step - loss: 1.5198\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 5s 668ms/step - loss: 1.5019\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 5s 666ms/step - loss: 1.5172\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 6s 694ms/step - loss: 1.4736\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 6s 697ms/step - loss: 1.4546\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 6s 686ms/step - loss: 1.4388\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 6s 704ms/step - loss: 1.4357\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 5s 668ms/step - loss: 1.4081\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 5s 665ms/step - loss: 1.3923\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 6s 694ms/step - loss: 1.4019\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 6s 701ms/step - loss: 1.3615\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 6s 704ms/step - loss: 1.3444\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 5s 656ms/step - loss: 1.3283\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 6s 677ms/step - loss: 1.3644\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 6s 684ms/step - loss: 1.3039\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 4s 489ms/step - loss: 1.2799\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 4s 448ms/step - loss: 1.2629\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 4s 456ms/step - loss: 1.2502\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 4s 441ms/step - loss: 1.2371\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 4s 437ms/step - loss: 1.2160\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 4s 442ms/step - loss: 1.1989\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 4s 449ms/step - loss: 1.1843\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 4s 449ms/step - loss: 1.1670\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 4s 473ms/step - loss: 1.1503\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 4s 456ms/step - loss: 1.1356\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 4s 452ms/step - loss: 1.1450\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 4s 448ms/step - loss: 1.1082\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 4s 507ms/step - loss: 1.0893\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 4s 522ms/step - loss: 1.0738\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 4s 501ms/step - loss: 1.0597\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 4s 471ms/step - loss: 1.0752\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 4s 448ms/step - loss: 1.0314\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 4s 441ms/step - loss: 1.0173\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 4s 450ms/step - loss: 1.0032\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 4s 460ms/step - loss: 0.9896\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 4s 477ms/step - loss: 0.9751\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 4s 513ms/step - loss: 0.9605\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 4s 467ms/step - loss: 0.9497\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 4s 462ms/step - loss: 0.9715\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 4s 470ms/step - loss: 0.9257\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 4s 474ms/step - loss: 0.9094\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 4s 506ms/step - loss: 0.8962\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 4s 514ms/step - loss: 0.8857\n",
      "Epoch 75/200\n",
      "8/8 [==============================] - 4s 490ms/step - loss: 0.8718\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 4s 483ms/step - loss: 0.8613\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 4s 470ms/step - loss: 0.8499\n",
      "Epoch 78/200\n",
      "8/8 [==============================] - 4s 554ms/step - loss: 0.8370\n",
      "Epoch 79/200\n",
      "8/8 [==============================] - 4s 489ms/step - loss: 0.8244\n",
      "Epoch 80/200\n",
      "8/8 [==============================] - 4s 537ms/step - loss: 0.8491\n",
      "Epoch 81/200\n",
      "8/8 [==============================] - 4s 511ms/step - loss: 0.8001\n",
      "Epoch 82/200\n",
      "8/8 [==============================] - 4s 474ms/step - loss: 0.7878\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 4s 467ms/step - loss: 0.7771\n",
      "Epoch 84/200\n",
      "8/8 [==============================] - 4s 474ms/step - loss: 0.7734\n",
      "Epoch 85/200\n",
      "8/8 [==============================] - 4s 465ms/step - loss: 0.7572\n",
      "Epoch 86/200\n",
      "8/8 [==============================] - 4s 479ms/step - loss: 0.7447\n",
      "Epoch 87/200\n",
      "8/8 [==============================] - 4s 463ms/step - loss: 0.7338\n",
      "Epoch 88/200\n",
      "8/8 [==============================] - 4s 457ms/step - loss: 0.7413\n",
      "Epoch 89/200\n",
      "8/8 [==============================] - 4s 492ms/step - loss: 0.7477\n",
      "Epoch 90/200\n",
      "8/8 [==============================] - 4s 464ms/step - loss: 0.7058\n",
      "Epoch 91/200\n",
      "8/8 [==============================] - 4s 482ms/step - loss: 0.6935\n",
      "Epoch 92/200\n",
      "8/8 [==============================] - 4s 478ms/step - loss: 0.6828\n",
      "Epoch 93/200\n",
      "8/8 [==============================] - 4s 464ms/step - loss: 0.6723\n",
      "Epoch 94/200\n",
      "8/8 [==============================] - 4s 462ms/step - loss: 0.6633\n",
      "Epoch 95/200\n",
      "8/8 [==============================] - 4s 484ms/step - loss: 0.6513\n",
      "Epoch 96/200\n",
      "8/8 [==============================] - 4s 466ms/step - loss: 0.6400\n",
      "Epoch 97/200\n",
      "8/8 [==============================] - 4s 534ms/step - loss: 0.6302\n",
      "Epoch 98/200\n",
      "8/8 [==============================] - 4s 461ms/step - loss: 0.6355\n",
      "Epoch 99/200\n",
      "8/8 [==============================] - 4s 462ms/step - loss: 0.6099\n",
      "Epoch 100/200\n",
      "8/8 [==============================] - 4s 461ms/step - loss: 0.6002\n",
      "Epoch 101/200\n",
      "8/8 [==============================] - 4s 453ms/step - loss: 0.5907\n",
      "Epoch 102/200\n",
      "8/8 [==============================] - 4s 456ms/step - loss: 0.5813\n",
      "Epoch 103/200\n",
      "8/8 [==============================] - 4s 457ms/step - loss: 0.5720\n",
      "Epoch 104/200\n",
      "8/8 [==============================] - 4s 456ms/step - loss: 0.5635\n",
      "Epoch 105/200\n",
      "8/8 [==============================] - 4s 465ms/step - loss: 0.5532\n",
      "Epoch 106/200\n",
      "8/8 [==============================] - 4s 467ms/step - loss: 0.5556\n",
      "Epoch 107/200\n",
      "8/8 [==============================] - 4s 481ms/step - loss: 0.5346\n",
      "Epoch 108/200\n",
      "8/8 [==============================] - 4s 459ms/step - loss: 0.5262\n",
      "Epoch 109/200\n",
      "8/8 [==============================] - 4s 461ms/step - loss: 0.5168\n",
      "Epoch 110/200\n",
      "8/8 [==============================] - 4s 454ms/step - loss: 0.5075\n",
      "Epoch 111/200\n",
      "8/8 [==============================] - 4s 454ms/step - loss: 0.5009\n",
      "Epoch 112/200\n",
      "8/8 [==============================] - 4s 461ms/step - loss: 0.5087\n",
      "Epoch 113/200\n",
      "8/8 [==============================] - 4s 460ms/step - loss: 0.4831\n",
      "Epoch 114/200\n",
      "8/8 [==============================] - 4s 456ms/step - loss: 0.4753\n",
      "Epoch 115/200\n",
      "8/8 [==============================] - 4s 452ms/step - loss: 0.4692\n",
      "Epoch 116/200\n",
      "8/8 [==============================] - 4s 454ms/step - loss: 0.4610\n",
      "Epoch 117/200\n",
      "8/8 [==============================] - 4s 455ms/step - loss: 0.4540\n",
      "Epoch 118/200\n",
      "8/8 [==============================] - 4s 495ms/step - loss: 0.4458\n",
      "Epoch 119/200\n",
      "8/8 [==============================] - 4s 476ms/step - loss: 0.4394\n",
      "Epoch 120/200\n",
      "8/8 [==============================] - 4s 457ms/step - loss: 0.4370\n",
      "Epoch 121/200\n",
      "8/8 [==============================] - 4s 467ms/step - loss: 0.4262\n",
      "Epoch 122/200\n",
      "8/8 [==============================] - 4s 456ms/step - loss: 0.4159\n",
      "Epoch 123/200\n",
      "8/8 [==============================] - 4s 458ms/step - loss: 0.4123\n",
      "Epoch 124/200\n",
      "8/8 [==============================] - 4s 467ms/step - loss: 0.4015\n",
      "Epoch 125/200\n",
      "8/8 [==============================] - 4s 462ms/step - loss: 0.3970\n",
      "Epoch 126/200\n",
      "8/8 [==============================] - 4s 459ms/step - loss: 0.3898\n",
      "Epoch 127/200\n",
      "8/8 [==============================] - 4s 459ms/step - loss: 0.3825\n",
      "Epoch 128/200\n",
      "8/8 [==============================] - 4s 465ms/step - loss: 0.3757\n",
      "Epoch 129/200\n",
      "8/8 [==============================] - 4s 465ms/step - loss: 0.3702\n",
      "Epoch 130/200\n",
      "8/8 [==============================] - 4s 473ms/step - loss: 0.3646\n",
      "Epoch 131/200\n",
      "8/8 [==============================] - 4s 460ms/step - loss: 0.3583\n",
      "Epoch 132/200\n",
      "8/8 [==============================] - 4s 456ms/step - loss: 0.3512\n",
      "Epoch 133/200\n",
      "8/8 [==============================] - 4s 462ms/step - loss: 0.3444\n",
      "Epoch 134/200\n",
      "8/8 [==============================] - 4s 454ms/step - loss: 0.3383\n",
      "Epoch 135/200\n",
      "8/8 [==============================] - 4s 459ms/step - loss: 0.3319\n",
      "Epoch 136/200\n",
      "8/8 [==============================] - 4s 460ms/step - loss: 0.3269\n",
      "Epoch 137/200\n",
      "8/8 [==============================] - 4s 457ms/step - loss: 0.3218\n",
      "Epoch 138/200\n",
      "8/8 [==============================] - 4s 460ms/step - loss: 0.3145\n",
      "Epoch 139/200\n",
      "8/8 [==============================] - 4s 461ms/step - loss: 0.3115\n",
      "Epoch 140/200\n",
      "8/8 [==============================] - 4s 456ms/step - loss: 0.3031\n",
      "Epoch 141/200\n",
      "8/8 [==============================] - 4s 455ms/step - loss: 0.2979\n",
      "Epoch 142/200\n",
      "8/8 [==============================] - 4s 459ms/step - loss: 0.2948\n",
      "Epoch 143/200\n",
      "8/8 [==============================] - 4s 461ms/step - loss: 0.2875\n",
      "Epoch 144/200\n",
      "8/8 [==============================] - 4s 471ms/step - loss: 0.2859\n",
      "Epoch 145/200\n",
      "8/8 [==============================] - 4s 479ms/step - loss: 0.2814\n",
      "Epoch 146/200\n",
      "8/8 [==============================] - 4s 467ms/step - loss: 0.2744\n",
      "Epoch 147/200\n",
      "8/8 [==============================] - 4s 493ms/step - loss: 0.2697\n",
      "Epoch 148/200\n",
      "8/8 [==============================] - 4s 495ms/step - loss: 0.2648\n",
      "Epoch 149/200\n",
      "8/8 [==============================] - 4s 469ms/step - loss: 0.2927\n",
      "Epoch 150/200\n",
      "8/8 [==============================] - 4s 466ms/step - loss: 0.2610\n",
      "Epoch 151/200\n",
      "8/8 [==============================] - 4s 459ms/step - loss: 0.2519\n",
      "Epoch 152/200\n",
      "8/8 [==============================] - 4s 466ms/step - loss: 0.2470\n",
      "Epoch 153/200\n",
      "8/8 [==============================] - 4s 462ms/step - loss: 0.2434\n",
      "Epoch 154/200\n",
      "8/8 [==============================] - 4s 458ms/step - loss: 0.2396\n",
      "Epoch 155/200\n",
      "8/8 [==============================] - 4s 461ms/step - loss: 0.2372\n",
      "Epoch 156/200\n",
      "8/8 [==============================] - 4s 463ms/step - loss: 0.2363\n",
      "Epoch 157/200\n",
      "8/8 [==============================] - 4s 461ms/step - loss: 0.2307\n",
      "Epoch 158/200\n",
      "8/8 [==============================] - 4s 467ms/step - loss: 0.2241\n",
      "Epoch 159/200\n",
      "8/8 [==============================] - 4s 466ms/step - loss: 0.2203\n",
      "Epoch 160/200\n",
      "8/8 [==============================] - 4s 467ms/step - loss: 0.2205\n",
      "Epoch 161/200\n",
      "8/8 [==============================] - 4s 460ms/step - loss: 0.2140\n",
      "Epoch 162/200\n",
      "8/8 [==============================] - 4s 460ms/step - loss: 0.2123\n",
      "Epoch 163/200\n",
      "8/8 [==============================] - 4s 461ms/step - loss: 0.2097\n",
      "Epoch 164/200\n",
      "8/8 [==============================] - 4s 465ms/step - loss: 0.2101\n",
      "Epoch 165/200\n",
      "8/8 [==============================] - 4s 459ms/step - loss: 0.2011\n",
      "Epoch 166/200\n",
      "8/8 [==============================] - 4s 466ms/step - loss: 0.1977\n",
      "Epoch 167/200\n",
      "8/8 [==============================] - 4s 469ms/step - loss: 0.1951\n",
      "Epoch 168/200\n",
      "8/8 [==============================] - 4s 476ms/step - loss: 0.1919\n",
      "Epoch 169/200\n",
      "8/8 [==============================] - 4s 462ms/step - loss: 0.1909\n",
      "Epoch 170/200\n",
      "8/8 [==============================] - 4s 473ms/step - loss: 0.1866\n",
      "Epoch 171/200\n",
      "8/8 [==============================] - 4s 465ms/step - loss: 0.1838\n",
      "Epoch 172/200\n",
      "8/8 [==============================] - 4s 502ms/step - loss: 0.1817\n",
      "Epoch 173/200\n",
      "8/8 [==============================] - 4s 473ms/step - loss: 0.1785\n",
      "Epoch 174/200\n",
      "8/8 [==============================] - 4s 467ms/step - loss: 0.1733\n",
      "Epoch 175/200\n",
      "8/8 [==============================] - 4s 471ms/step - loss: 0.1703\n",
      "Epoch 176/200\n",
      "8/8 [==============================] - 4s 475ms/step - loss: 0.2030\n",
      "Epoch 177/200\n",
      "8/8 [==============================] - 4s 477ms/step - loss: 0.1665\n",
      "Epoch 178/200\n",
      "8/8 [==============================] - 4s 460ms/step - loss: 0.1670\n",
      "Epoch 179/200\n",
      "8/8 [==============================] - 4s 501ms/step - loss: 0.1603\n",
      "Epoch 180/200\n",
      "8/8 [==============================] - 4s 458ms/step - loss: 0.1578\n",
      "Epoch 181/200\n",
      "8/8 [==============================] - 4s 463ms/step - loss: 0.1564\n",
      "Epoch 182/200\n",
      "8/8 [==============================] - 4s 465ms/step - loss: 0.1532\n",
      "Epoch 183/200\n",
      "8/8 [==============================] - 4s 490ms/step - loss: 0.1542\n",
      "Epoch 184/200\n",
      "8/8 [==============================] - 4s 477ms/step - loss: 0.1520\n",
      "Epoch 185/200\n",
      "8/8 [==============================] - 4s 466ms/step - loss: 0.1463\n",
      "Epoch 186/200\n",
      "8/8 [==============================] - 4s 473ms/step - loss: 0.1439\n",
      "Epoch 187/200\n",
      "8/8 [==============================] - 4s 466ms/step - loss: 0.1426\n",
      "Epoch 188/200\n",
      "8/8 [==============================] - 4s 460ms/step - loss: 0.1430\n",
      "Epoch 189/200\n",
      "8/8 [==============================] - 4s 483ms/step - loss: 0.1527\n",
      "Epoch 190/200\n",
      "8/8 [==============================] - 4s 483ms/step - loss: 0.1362\n",
      "Epoch 191/200\n",
      "8/8 [==============================] - 4s 489ms/step - loss: 0.1342\n",
      "Epoch 192/200\n",
      "8/8 [==============================] - 4s 470ms/step - loss: 0.1321\n",
      "Epoch 193/200\n",
      "8/8 [==============================] - 4s 475ms/step - loss: 0.1308\n",
      "Epoch 194/200\n",
      "8/8 [==============================] - 4s 470ms/step - loss: 0.1304\n",
      "Epoch 195/200\n",
      "8/8 [==============================] - 4s 527ms/step - loss: 0.1266\n",
      "Epoch 196/200\n",
      "8/8 [==============================] - 4s 521ms/step - loss: 0.1240\n",
      "Epoch 197/200\n",
      "8/8 [==============================] - 4s 546ms/step - loss: 0.1221\n",
      "Epoch 198/200\n",
      "8/8 [==============================] - 4s 513ms/step - loss: 0.1237\n",
      "Epoch 199/200\n",
      "8/8 [==============================] - 4s 522ms/step - loss: 0.1173\n",
      "Epoch 200/200\n",
      "8/8 [==============================] - 4s 479ms/step - loss: 0.1157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x298664c3e80>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([enc_input_data, dec_input_data],dec_out_data, epochs = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "67ea1b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('seq2seq_mod2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "f65c3bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens(sentence: str):\n",
    "    # convert input string to lowercase, \n",
    "    # then split it by whitespaces\n",
    "    tokenizer = Tokenizer(filters = '', oov_token = '<OOV>')\n",
    "    tokenizer.fit_on_texts([self.preprocess_sentence(q_) for q_ in questions] + [self.preprocess_sentence(a_, answer = True) for a_ in answers])\n",
    "    words = sentence.lower().split()\n",
    "    # and then convert to a sequence \n",
    "    # of integers padded with zeros\n",
    "    tokens_list = list()\n",
    "    for current_word in words:\n",
    "        result = tokenizer.word_index.get(current_word, '')\n",
    "        if result != '':\n",
    "            tokens_list.append(result)\n",
    "    return pad_sequences([tokens_list],\n",
    "                         maxlen=maxlen_q,\n",
    "                         padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "5c067a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    # two inputs for the state vectors returned by encoder\n",
    "    dec_state_input_h = Input(shape=(200,))\n",
    "    dec_state_input_c = Input(shape=(200,))\n",
    "    dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
    "    # these state vectors are used as an initial state \n",
    "    # for LSTM layer in the inference decoder\n",
    "    # third input is the Embedding layer as explained above   \n",
    "    dec_outputs, state_h, state_c = dec_lstm(dec_embedding,\n",
    "                                    initial_state=dec_states_inputs)\n",
    "    dec_states = [state_h, state_c]\n",
    "    # Dense layer is used to return OHE predicted word\n",
    "    dec_outputs = dec_dense(dec_outputs)\n",
    "    dec_model = Model(\n",
    "        inputs=[dec_inputs] + dec_states_inputs,\n",
    "        outputs=[dec_outputs] + dec_states)\n",
    "   \n",
    "    # single encoder input is a question, represented as a sequence \n",
    "    # of integers padded with zeros\n",
    "    enc_model = Model(inputs=enc_inputs, outputs=enc_states)\n",
    "   \n",
    "    return enc_model,  dec_model\n",
    "enc_model, dec_model = make_inference_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0417f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_processed = [data_creator.preprocess_sentence(q_) for q_ in questions]\n",
    "a_processed = [data_creator.preprocess_sentence(a_, answer = True) for a_ in answers]\n",
    "tokenizer_s = Tokenizer(filters = '', oov_token = '<OOV>')\n",
    "tokenizer_s.fit_on_texts(q_processed + a_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53594fe",
   "metadata": {},
   "source": [
    "## Main interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "21c9aaad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When does regularization come into play in Machine Learning?\n",
      " parameters use variance performance between value test then dataset s bayes function but distribution how when will so based accuracy time mean validation element supervised case true also well very parameters class algorithms some different random would most b about a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a there test model be is is is is discard observed function but distribution how when will so based accuracy on you used set one error given bias type features there using do classifier classification number better find difference decision method list has them these no any just called split could\n"
     ]
    }
   ],
   "source": [
    "#print('***---Welcome to the Data Science and Machine Learning interview---***')    \n",
    "#print('Are you ready for the interview? (Y/N)')\n",
    "#res = input()\n",
    "#if res == \"Y\" or \"y\":\n",
    " #   print('Lets start the interview... \\n')\n",
    "  #  num = 5  # Defining the number of questions to be asked in the interview\n",
    "   # q_list = []\n",
    "    #count = 0\n",
    "    #if count <= num:\n",
    "    \n",
    "    \n",
    "ques = random.choice(data.Questions)\n",
    "print(ques)\n",
    "   # if ques not in q_list:\n",
    "    #    q_list.append(ques)\n",
    "     #   print(f'{count +1 } ', ques)\n",
    "\n",
    "        # encode the input sequence into state vectors\n",
    "states_values = enc_model.predict(\n",
    "    data_creator.str_to_tokens(ques))\n",
    "# start with a target sequence of size 1 - word 'start'   \n",
    "empty_target_seq = np.zeros((1, 1))\n",
    "empty_target_seq[0, 0] = tokenizer_s.word_index['start']\n",
    "stop_condition = False\n",
    "decoded_translation = ''\n",
    "while not stop_condition:\n",
    "    # feed the state vectors and 1-word target sequence \n",
    "    # to the decoder to produce predictions for the next word\n",
    "    dec_outputs, h, c = dec_model.predict([empty_target_seq] \n",
    "                                          + states_values)         \n",
    "    # sample the next word using these predictions\n",
    "    sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "    sampled_word = None\n",
    "    # append the sampled word to the target sequence\n",
    "    for word, index in tokenizer_s.word_index.items():\n",
    "        if sampled_word_index == index:\n",
    "            if word != 'end':\n",
    "                decoded_translation += ' {}'.format(word)\n",
    "            sampled_word = word\n",
    "    # repeat until we generate the end-of-sequence word 'end' \n",
    "    # or we hit the length of answer limit\n",
    "    if sampled_word == 'end' \\\n",
    "            or len(decoded_translation.split()) \\\n",
    "            > 150:\n",
    "        stop_condition = True\n",
    "            # prepare next iteration\n",
    "    empty_target_seq = np.zeros((1, 1))\n",
    "    empty_target_seq[0, 0] = sampled_word_index\n",
    "    states_values = [h, c]\n",
    "print(decoded_translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5eab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4265267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
